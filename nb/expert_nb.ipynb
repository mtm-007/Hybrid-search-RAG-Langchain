{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain_community.llms import CTransformers\n",
    "#from ctransformers import CTransformers\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from wandb.integration.openai import autolog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WANDB_NOTEBOOK_NAME = 'expermentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"llmapps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start logging to W&B\n",
    "# from wandb.integration.huggingface import autolog\n",
    "# autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key= os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = key\n",
    "PINECONE_API_ENV = 'gcp-starter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data func\n",
    "def load_pdf_data(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob='*.pdf',\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../'\n",
    "extracted_data = load_pdf_data(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks of text\n",
    "def text_chunk_splitter(extracted_data):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunk = text_split.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_chunk_splitter(extracted_data)\n",
    "print(\"chunk length:  \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_embedding_model():\n",
    "    embedding= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = download_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "#     api_key=\"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\",\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = Chroma.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = \"chroma_db\"\n",
    "\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=text_chunks, embedding=embeddings, persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "# vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = embedding.embed_query(\"Hi today is March 12 Tuesday\")\n",
    "print(\"Length: \", len(query_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.Pinecone(\n",
    "   api_key=os.getenv(\"PINECONE_API_KEY\"),  \n",
    "   environment=os.getenv(\"PINECONE_ENV\"),  \n",
    ")\n",
    "index_name = \"medical-chatbot-vector-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line when loading the embedding to the vector store first time\n",
    "docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embedding)\n",
    "query = \"what is allergy?\"\n",
    "retiriever = docsearch.as_retriever(search_kwargs=dict(k=3))\n",
    "docs = retiriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"Results\", docs)\n",
    "#docs[0].page_content\n",
    "#[doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in docs:\n",
    "#     print(doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = docs.as_retriever(search_kwargs=dict(k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" \n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "\n",
    "Context : {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "#PROMPT = ChatPromptTemplate(prompt_template) #,input_variables=[\"context\", \"question\"])\n",
    "\n",
    "#prompt = PROMPT.format(context=context,question= query) \n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-Chat-GGML\", model_file=\"llama-2-7b-chat.ggmlv3.q3_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ctransformers import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model= \"../model/gguf/llama-2-7b-chat.Q3_K_M.gguf\",\n",
    "                    model_type=\"llama\",\n",
    "                    config={'max_new_tokens': 1024,\n",
    "                            'temperature': 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA = RetrievalQA.from_llm(llm=llm, \n",
    "#                                  chain_type=\"stuff\",\n",
    "#                                  retriever = docsearch.as_retriever(search_kwargs={'k':3})\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.predict(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retiriever=retiriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = retiriever,\n",
    "    #retriever = docsearch.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoConfig\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\")\n",
    "config.config.max_new_tokens = 2000\n",
    "config.config.context_length = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "while True:\n",
    "    user_input = input(f\"Input prompt: \")\n",
    "    if user_input == 'exit':\n",
    "        print(\"Exiting\")\n",
    "        sys.exit()\n",
    "    if user_input == ' ':\n",
    "        continue\n",
    "    result = QA({'query': user_input})\n",
    "    print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_Input: input(f\"Input prompt: \")\n",
    "#     result = QA({\"query\": user_Input})\n",
    "#     print(\"Response: \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
