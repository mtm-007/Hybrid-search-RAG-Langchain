{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_community.llms import CTransformers\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key= os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = key\n",
    "PINECONE_API_ENV = 'gcp-starter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data func\n",
    "def load_pdf_data(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob='*.pdf',\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../'\n",
    "extracted_data = load_pdf_data(\"../data/yt_test/bigCorps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks of text\n",
    "def text_chunk_splitter(extracted_data):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunk = text_split.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk length:   2873\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_chunk_splitter(extracted_data)\n",
    "print(\"chunk length:  \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"1\\n00:00:00,120 --> 00:01:01,119\\nthis speaker outperformed Tony Robbins last year he was more popular and people love Tony\\nRobbins it's no put in not sailing against Tony he's amazing I love the guy but people said he was\\nbetter so let me ask you a question imagine you're living in your dream home you got the cabin for\\nthe family the beach house your bills are paid you've built something big but you don't have your\" metadata={'source': '../data/yt_test/bigCorps/gary_Big_timeSt_ytapi-v1.pdf', 'page': 0}\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0]);print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_embedding_model():\n",
    "    embedding= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = download_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  384\n"
     ]
    }
   ],
   "source": [
    "query_res = embedding.embed_query(\"Hi today is March 12 Tuesday\")\n",
    "print(\"Length: \", len(query_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.Pinecone(\n",
    "   api_key=os.getenv(\"PINECONE_API_KEY\"),  \n",
    "   environment=os.getenv(\"PINECONE_ENV\"),  \n",
    ")\n",
    "index_name = \"gary-big-corps-chatbot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this line when loading the embedding to the vector store first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line when loading the embedding to the vector store first time\n",
    "docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results [Document(page_content=\"effective insulin I mean insulin resistance I can't even tell you how many conditions that leads to by\\nthe way do you know that we are the um one of the few civilized nations in the world that continues\\nto call Alzheimer's and Dementia Alzheimer's and Dementia does anybody know what it is\\n85\\n00:39:49,440 --> 00:40:49,198\"), Document(page_content=\"resistance AKA metabolic dysfunction right even the American Heart Association admits that half of\\nAmericans are either pre-diabetic undiagnosed diabetes or diabetes over 150 million people\\naccording to the\\n77\\n00:32:53,320 --> 00:33:56,599\\nAmerican Heart Association have some form of insulin resistance metabolic dysfunction wow I\\nwould argue it's closer to 90% but even the AHA claims it's over 150 million of 330 million plus\"), Document(page_content='insulin but the industry wants to get you dependent on insulin do you know that diabetes is a 110')]\n"
     ]
    }
   ],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embedding)\n",
    "query = \"what is insulin resistance?\"\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "retiriever = docsearch.as_retriever(search_kwargs=dict(k=3))\n",
    "docs = retiriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"Results\", docs)\n",
    "#docs[0].page_content\n",
    "#[doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" \n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "\n",
    "Context : {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "#PROMPT = ChatPromptTemplate(prompt_template) #,input_variables=[\"context\", \"question\"])\n",
    "\n",
    "#prompt = PROMPT.format(context=context,question= query) \n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel\n",
    "\n",
    "# class BaseCallbackManager(BaseModel):\n",
    "#     # Define your fields here\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.B\n",
    "- CMAKE_ARGS=\"-DLLAMA_OPENBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.56\n",
    "\n",
    "- CMAKE_ARGS=\"-DLLAMA_METAL_EMBED_LIBRARY=ON -DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../model/gguf/llama-2-7b-chat.Q3_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q4_K:   92 tensors\n",
      "llama_model_loader: - type q5_K:    4 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3090.83 MiB, ( 9927.36 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3090.82 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/mtm007/Downloads/LLM/Medchatbot/RAG_MedChatBot/Rchat/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2048.00 MiB, (11980.36 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    17.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   296.02 MiB, (12276.38 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '12', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path= \"../model/gguf/llama-2-7b-chat.Q3_K_M.gguf\",\n",
    "    n_gpu_layers= -1,\n",
    "    n_batch= 512,\n",
    "    max_tokens= 512,\n",
    "    n_ctx = 4096, \n",
    "    #context_length= 4096,\n",
    "    temperature=0.7,\n",
    "    callback_manager=callback_manager,\n",
    "    #verbose=True # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm, \n",
    "#     retriever = retiriever,\n",
    "#     chain_type=\"stuff\",\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     return_source_documents=True\n",
    "# )\n",
    "QA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = retiriever,\n",
    "    #retriever = docsearch.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtm007/Downloads/LLM/Medchatbot/RAG_MedChatBot/Rchat/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insulin resistance occurs when cells within the body become less responsive to insulin, leading to high blood sugar levels. Insulin resistance can occur due to various factors including genetics, obesity, physical inactivity, and poor diet. Insulin resistance can lead to conditions such as type 2 diabetes, metabolic syndrome, and cardiovascular disease. Treatment options for insulin resistance include lifestyle modifications such as weight loss, regular exercise, and a healthy diet, as well as medications such as metformin. It is important"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18914.03 ms\n",
      "llama_print_timings:      sample time =      50.36 ms /   129 runs   (    0.39 ms per token,  2561.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18913.01 ms /   383 tokens (   49.38 ms per token,    20.25 tokens per second)\n",
      "llama_print_timings:        eval time =   32167.58 ms /   128 runs   (  251.31 ms per token,     3.98 tokens per second)\n",
      "llama_print_timings:       total time =   52333.51 ms /   511 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Insulin resistance occurs when cells within the body become less responsive to insulin, leading to high blood sugar levels. Insulin resistance can occur due to various factors including genetics, obesity, physical inactivity, and poor diet. Insulin resistance can lead to conditions such as type 2 diabetes, metabolic syndrome, and cardiovascular disease. Treatment options for insulin resistance include lifestyle modifications such as weight loss, regular exercise, and a healthy diet, as well as medications such as metformin. It is important'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is insulin resistance?\"\n",
    "result = QA({'query': query})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input prompt:  what is insulin resistance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insulin resistance is a condition where the body's cells become less responsive to insulin, a hormone produced by the pancreas that regulates blood sugar levels. Insulin resistance occurs when the body's cells are unable to use insulin effectively, leading to high blood sugar levels. This can lead to a range of health problems, including type 2 diabetes, metabolic syndrome, and cardiovascular disease. Insulin resistance can be caused by a variety of factors, including genetics, obesity, physical inactivity, and poor diet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   18914.03 ms\n",
      "llama_print_timings:      sample time =      51.02 ms /   129 runs   (    0.40 ms per token,  2528.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   35787.30 ms /   129 runs   (  277.42 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:       total time =   36816.29 ms /   130 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Insulin resistance is a condition where the body's\n",
      "cells become less responsive to insulin, a hormone\n",
      "produced by the pancreas that regulates blood\n",
      "sugar levels. Insulin resistance occurs when the\n",
      "body's cells are unable to use insulin\n",
      "effectively, leading to high blood sugar levels.\n",
      "This can lead to a range of health problems,\n",
      "including type 2 diabetes, metabolic syndrome, and\n",
      "cardiovascular disease. Insulin resistance can be\n",
      "caused by a variety of factors, including\n",
      "genetics, obesity, physical inactivity, and poor\n",
      "diet\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input prompt:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtm007/Downloads/LLM/Medchatbot/RAG_MedChatBot/Rchat/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "while True:\n",
    "    user_input = input(f\"Input prompt: \")\n",
    "    if user_input == 'exit':\n",
    "        print(\"Exiting\")\n",
    "        sys.exit()\n",
    "    if user_input == ' ':\n",
    "        continue\n",
    "    result = QA({'query': user_input})\n",
    "    result = textwrap.fill(result[\"result\"], width=50)\n",
    "    #print(f\"Answer: {wraped_result['wraped_result']}\")\n",
    "    print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     user_Input: input(f\"Input prompt: \")\n",
    "#     result = QA({\"query\": user_Input})\n",
    "#     print(\"Response: \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ValueError: Document prompt requires documents to have metadata variables: ['source']. Received document with missing metadata: ['source']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = retiriever,\n",
    "    chain_type=\"stuff\",\n",
    "    #chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa_sources(query)----refer to the above value error, the source document needs improved data ingestion and procesing toninclude metadata like source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
