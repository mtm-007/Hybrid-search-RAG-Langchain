{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain.llms import CTransformers\n",
    "from langchain_community.llms import CTransformers\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key= os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = key\n",
    "PINECONE_API_ENV = 'gcp-starter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data func\n",
    "def load_pdf_data(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob='*.pdf',\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_data(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks of text\n",
    "def text_chunk_splitter(extracted_data):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    text_chunk = text_split.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_chunk_splitter(extracted_data)\n",
    "print(\"chunk length:  \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_embedding_model():\n",
    "    embedding= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = download_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = embedding.embed_query(\"Hi today is March 12 Tuesday\")\n",
    "print(\"Length: \", len(query_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.Pinecone(\n",
    "   api_key=os.getenv(\"PINECONE_API_KEY\"),  \n",
    "   environment=os.getenv(\"PINECONE_ENV\"),  \n",
    ")\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embedding)\n",
    "query = \"what is allergy?\"\n",
    "docs = docsearch.similarity_search(query,k=3)\n",
    "\n",
    "print(\"Results\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" \n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "\n",
    "Context : {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-Chat-GGML\", model_file=\"llama-2-7b-chat.ggmlv3.q3_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model= \"model/llama-2-7b-chat.ggmlv3.q3_K_M.bin\",\n",
    "                    model_type=\"llama\",\n",
    "                    config={'max_new_tokens': 512,\n",
    "                            'temperature': 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = docsearch.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input: input(f\"Input prompt: \")\n",
    "    result = QA({\"query\": user_input})\n",
    "    print(\"Response: \", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
